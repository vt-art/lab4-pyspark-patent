{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 4253 / 5253 - Lab #4 - Patent Problem with Spark RDD - SOLUTION\n",
    "<div>\n",
    " <h2> CSCI 4283 / 5253 \n",
    "  <IMG SRC=\"https://www.colorado.edu/cs/profiles/express/themes/cuspirit/logo.png\" WIDTH=50 ALIGN=\"right\"/> </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [Spark cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf) is useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=SparkConf().setAppName(\"Lab4-rdd\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PySpark and RDD's on the https://coding.csel.io machines is slow -- most of the code is executed in Python and this is much less efficient than the java-based code using the PySpark dataframes. Be patient and trying using `.cache()` to cache the output of joins. You may want to start with a reduced set of data before running the full task. You can use the `sample()` method to extract just a sample of the data or use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two RDD's are called \"rawCitations\" and \"rawPatents\" because you probably want to process them futher (e.g. convert them to integer types, etc). \n",
    "\n",
    "The `textFile` function returns data in strings. This should work fine for this lab.\n",
    "\n",
    "Other methods you use might return data in type `Byte`. If you haven't used Python `Byte` types before, google it. You can convert a value of `x` type byte into e.g. a UTF8 string using `x.decode('uft-8')`. Alternatively, you can use the `open` method of the gzip library to read in all the lines as UTF-8 strings like this:\n",
    "```\n",
    "import gzip\n",
    "with gzip.open('cite75_99.txt.gz', 'rt',encoding='utf-8') as f:\n",
    "    rddCitations = sc.parallelize( f.readlines() )\n",
    "```\n",
    "This is less efficient than using `textFile` because `textFile` would use the underlying HDFS or other file system to read the file across all the worker nodes while the using `gzip.open()...readlines()` will read all the data in the frontend and then distribute it to all the worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc for Spark Content will create the RDD\n",
    "rddCitations = sc.textFile(\"cite75_99.txt.gz\")\n",
    "rddPatents = sc.textFile(\"apat63_99.txt.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"CITING\",\"CITED\"',\n",
       " '3858241,956203',\n",
       " '3858241,1324234',\n",
       " '3858241,3398406',\n",
       " '3858241,3557384']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCitations.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"PATENT\",\"GYEAR\",\"GDATE\",\"APPYEAR\",\"COUNTRY\",\"POSTATE\",\"ASSIGNEE\",\"ASSCODE\",\"CLAIMS\",\"NCLASS\",\"CAT\",\"SUBCAT\",\"CMADE\",\"CRECEIVE\",\"RATIOCIT\",\"GENERAL\",\"ORIGINAL\",\"FWDAPLAG\",\"BCKGTLAG\",\"SELFCTUB\",\"SELFCTLB\",\"SECDUPBD\",\"SECDLWBD\"',\n",
       " '3070801,1963,1096,,\"BE\",\"\",,1,,269,6,69,,1,,0,,,,,,,',\n",
       " '3070802,1963,1096,,\"US\",\"TX\",,1,,2,6,63,,0,,,,,,,,,',\n",
       " '3070803,1963,1096,,\"US\",\"IL\",,1,,2,6,63,,9,,0.3704,,,,,,,',\n",
       " '3070804,1963,1096,,\"US\",\"OH\",,1,,2,6,63,,3,,0.6667,,,,,,,']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddPatents.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, they are a single string with multiple CSV's. You will need to convert these to (K,V) pairs, probably convert the keys to `int` and so on. You'll need to `filter` out the header string as well since there's no easy way to extract all the lines except the first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach:\n",
    "1) Parse the csv file from rddPatents into two RDDs.\n",
    "- The first one `patents_all` includes all columns of rddPatents, stored as a Python list per record. This RDD will be joined with the co-state counts at the end of the code so that the resulting database includes all of the columns from patents (p.*) along with the co-state citation counts.\n",
    "- The second one `patents_filtered` subsets `patents_all` to those records that are not missing the state variable (`POSTATE`). This is necessary because we cannot produce the co-state citation count when the state when this variable is missing. This RDD is also limited to only the columns we need in order to make joining and filtering more efficient.\n",
    "- The parsed RDD do not contain the header row.\n",
    "- The outputted patent numbers are now integers instead of strings, which produces consistent numeric keys.\n",
    "- When creating the RDD, partition into 200 chucks using `partitionBy(nump)` where `nump=200`, so that the operations can run in parallel. Specifically, `.partitionBy(numP)` will hash-partition the key–value pairs so that all records with the same key end up in the same partition. This enables efficient joins by minimizing data shuffles.\n",
    "- `.persist(StorageLevel.MEMORY_AND_DISK)` keeps the RDD in cache so that it does not need to be recomputed when it is used later in the processing.\n",
    "\n",
    "2) Parse the rddCitations csv file into one new RDD.\n",
    "- The citations RDD is parsed the same way as the patents above, regarding removing the header row, partitioning and cache.\n",
    "- Removed any rows with missing CITED or CITING patent numbers.\n",
    "- Output CITING and CITED as integers instead of strings.\n",
    "\n",
    "3) Map the variables on citations to swap them so that we can join on CITED. Use partitioning.\n",
    "4) Join with `patents_filtered` with key = CITED to add the cited patent's state to each citation record.\n",
    "5) Re-key by CITING to join to get the citing patent's state. This will let us use the citing patent's number.\n",
    "6) Join with `patents_filtered` with key = CITING to add the citing patent's state to each record.\n",
    "7) Filter to records where the cited state is equal to the citing state.\n",
    "8) Count co-state citations per citing patent. Use reduceByKey to count co-state citations per citing patent.\n",
    "9) Finally, complete a left join where `patents_all` (left) is joined with the co-state citation counts.\n",
    "10) Construct the final output rows by creating a single list per record that includes all columns plus the count.\n",
    "11) Create the table\n",
    "- Use `.takeOrdered` to retrieve the top 10 patents by co-state citation count.\n",
    "- Get the header from `rddPatents`.\n",
    "- Organize the data into a table using a function `show_table`.\n",
    "- Show the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+---------+---------+---------+----------+---------+--------+--------+-----+--------+-------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+------------+\n",
      "| PATENT  | GYEAR | GDATE | APPYEAR | COUNTRY | POSTATE | ASSIGNEE | ASSCODE | CLAIMS | NCLASS | CAT | SUBCAT | CMADE | CRECEIVE | RATIOCIT | GENERAL | ORIGINAL | FWDAPLAG | BCKGTLAG | SELFCTUB | SELFCTLB | SECDUPBD | SECDLWBD | SAME_STATE |\n",
      "+---------+-------+-------+---------+---------+---------+----------+---------+--------+--------+-----+--------+-------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+------------+\n",
      "| 5959466 | 1999  | 14515 | 1997    | US      | CA      | 5310     | 2       | null   | 326    | 4   | 46     | 159   | 0        | 1        | null    | 0.6186   | null     | 4.8868   | 0.0455   | 0.044    | null     | null     | 125        |\n",
      "| 5983822 | 1999  | 14564 | 1998    | US      | TX      | 569900   | 2       | null   | 114    | 5   | 55     | 200   | 0        | 0.995    | null    | 0.7201   | null     | 12.45    | 0        | 0        | null     | null     | 103        |\n",
      "| 6008204 | 1999  | 14606 | 1998    | US      | CA      | 749584   | 2       | null   | 514    | 3   | 31     | 121   | 0        | 1        | null    | 0.7415   | null     | 5        | 0.0085   | 0.0083   | null     | null     | 100        |\n",
      "| 5952345 | 1999  | 14501 | 1997    | US      | CA      | 749584   | 2       | null   | 514    | 3   | 31     | 118   | 0        | 1        | null    | 0.7442   | null     | 5.1102   | 0        | 0        | null     | null     | 98         |\n",
      "| 5998655 | 1999  | 14585 | 1998    | US      | CA      | null     | 1       | null   | 560    | 1   | 14     | 114   | 0        | 1        | null    | 0.7387   | null     | 5.1667   | null     | null     | null     | null     | 96         |\n",
      "| 5958954 | 1999  | 14515 | 1997    | US      | CA      | 749584   | 2       | null   | 514    | 3   | 31     | 116   | 0        | 1        | null    | 0.7397   | null     | 5.181    | 0        | 0        | null     | null     | 96         |\n",
      "| 5936426 | 1999  | 14466 | 1997    | US      | CA      | 5310     | 2       | null   | 326    | 4   | 46     | 178   | 0        | 1        | null    | 0.58     | null     | 11.2303  | 0.0765   | 0.073    | null     | null     | 94         |\n",
      "| 5925042 | 1999  | 14445 | 1997    | US      | CA      | 733846   | 2       | null   | 606    | 3   | 32     | 242   | 0        | 1        | null    | 0.7382   | null     | 8.3471   | 0        | 0        | null     | null     | 90         |\n",
      "| 5913855 | 1999  | 14417 | 1997    | US      | CA      | 733846   | 2       | null   | 606    | 3   | 32     | 242   | 0        | 1        | null    | 0.7403   | null     | 8.3595   | 0        | 0        | null     | null     | 90         |\n",
      "| 5739256 | 1998  | 13983 | 1995    | US      | CA      | 70060    | 2       | 15     | 528    | 1   | 15     | 453   | 0        | 1        | null    | 0.8232   | null     | 15.1104  | 0.1124   | 0.1082   | null     | null     | 90         |\n",
      "+---------+-------+-------+---------+---------+---------+----------+---------+--------+--------+-----+--------+-------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from operator import add\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "def clean(s):\n",
    "    return \"\" if s is None else s.strip()\n",
    "\n",
    "def parse_patent_full(line):\n",
    "    row = next(csv.reader([line]))\n",
    "    if row and row[0].strip().upper().replace('\"','') == \"PATENT\":\n",
    "        return None\n",
    "    if len(row) <= 5:\n",
    "        return None\n",
    "    patent_str = row[0].strip().replace('\"','')\n",
    "    if patent_str == \"\":\n",
    "        return None\n",
    "    return (int(patent_str), row)  # (PATENT:int, full_row_list)\n",
    "\n",
    "def parse_citation_csv(line):\n",
    "    row = next(csv.reader([line]))\n",
    "    if row and row[0].strip().upper().replace('\"','') in (\"CITING\", \"CITPAT\"):\n",
    "        return None\n",
    "    if len(row) < 2:\n",
    "        return None\n",
    "    citing_str = row[0].strip().replace('\"','')\n",
    "    cited_str  = row[1].strip().replace('\"','')\n",
    "    if citing_str == \"\" or cited_str == \"\":\n",
    "        return None\n",
    "    return (int(citing_str), int(cited_str))  # (CITING:int, CITED:int)\n",
    "\n",
    "numP = 200\n",
    "\n",
    "# Elements of patents_all are (patent_id, full_row_list)\n",
    "# x[0] = PATENT (int)\n",
    "# x[1] = row (a Python list of all CSV fields)\n",
    "# where inside the row list is:\n",
    "# x[1][0] = \"PATENT\" value\n",
    "# x[1][1] = \"GYEAR\"\n",
    "# ...\n",
    "# x[1][5] = \"POSTATE\"\n",
    "patents_all = (rddPatents.map(parse_patent_full)\n",
    "               .filter(lambda x: x is not None)\n",
    "               .partitionBy(numP)\n",
    "               .persist(StorageLevel.MEMORY_AND_DISK))\n",
    "\n",
    "\n",
    "# For joins: only patents with non-empty POSTATE \n",
    "# Elements of patents_filtered are (patent_id, postate)\n",
    "# kv[0] = PATENT\n",
    "# kv[1] = POSTATE (trimmed)\n",
    "patents_filtered = (patents_all\n",
    "                    .mapValues(lambda row: clean(row[5]))\n",
    "                    .filter(lambda kv: kv[1] != \"\")\n",
    "                    .persist(StorageLevel.MEMORY_AND_DISK))\n",
    "\n",
    "\n",
    "# Elements of citations are (citing_patent, cited_patent)\n",
    "# x[0] = CITING\n",
    "# x[1] = CITED\n",
    "citations = (rddCitations.map(parse_citation_csv)\n",
    "             .filter(lambda x: x is not None)\n",
    "             .persist(StorageLevel.MEMORY_AND_DISK))\n",
    "\n",
    "\n",
    "# CITED -> cited_state\n",
    "# citations_by_cited swaps them so we can join on CITED.\n",
    "# Elements of citations_by_cited are (CITED, CITING)\n",
    "# x[0] = CITED\n",
    "# x[1] = CITING\n",
    "citations_by_cited = citations.map(lambda x: (x[1], x[0])).partitionBy(numP)\n",
    "\n",
    "\n",
    "# Join on key = CITED:\n",
    "# left side: (CITED, CITING)\n",
    "# right side: (CITED, cited_state)\n",
    "# After join, each element is (CITED, (CITING, cited_state))\n",
    "# x[0] = CITED\n",
    "# x[1] = (CITING, cited_state)\n",
    "# x[1][0] = CITING\n",
    "# x[1][1] = cited_state\n",
    "with_cited_state = citations_by_cited.join(patents_filtered)\n",
    "\n",
    "# CITING -> citing_state\n",
    "# Re-key by CITING to join to get the citing patent’s state.\n",
    "# by_citing elements are (CITING, cited_state)\n",
    "# x[0] = CITING\n",
    "# x[1] = cited_state\n",
    "by_citing = with_cited_state.map(lambda x: (x[1][0], x[1][1])).partitionBy(numP)\n",
    "\n",
    "# Join on key = CITING:\n",
    "# left: (CITING, cited_state)\n",
    "# right: (CITING, citing_state)\n",
    "# After join, each element is (CITING, (cited_state, citing_state))\n",
    "# x[0] = CITING\n",
    "# x[1] = (cited_state, citing_state)\n",
    "# x[1][0] = cited_state\n",
    "# x[1][1] = citing_state\n",
    "with_both_states = by_citing.join(patents_filtered)\n",
    "\n",
    "# State filter\n",
    "filtered = with_both_states.filter(lambda x: x[1][0] == x[1][1])\n",
    "\n",
    "# Count co-state citations per citing patent\n",
    "# Use reduceByKey to count co-state citations per citing patent\n",
    "# Each element of co_state_counts is (CITING, CO_CITED_COUNT)\n",
    "# x[0] = PATENT (citing patent id)\n",
    "# x[1] = count of co-state citations\n",
    "co_state_counts = (filtered\n",
    "                   .map(lambda x: (x[0], 1))\n",
    "                   .reduceByKey(add, numPartitions=numP)\n",
    "                   .persist(StorageLevel.MEMORY_AND_DISK))\n",
    "\n",
    "# LEFT JOIN onto all patents and fill missing with 0\n",
    "# Join key = PATENT:\n",
    "# left: (PATENT, full_row_list)\n",
    "# right: (PATENT, count)\n",
    "# Result: (PATENT, (full_row_list, count))\n",
    "# x[0] = PATENT\n",
    "# x[1] = (full_row_list, maybe_count)\n",
    "# x[1][0] = full patent row list\n",
    "# x[1][1] = count or None if no match\n",
    "joined = patents_all.leftOuterJoin(co_state_counts)\n",
    "\n",
    "# single list per record that includes all columns plus the count\n",
    "rows_with_count = joined.map(\n",
    "    lambda x: x[1][0] + [str(x[1][1] if x[1][1] is not None else 0)]\n",
    ")\n",
    "\n",
    "# Top 10 patents by co-state citation count\n",
    "result_rdd_final = rows_with_count.takeOrdered(10, key=lambda row: -int(row[-1]))\n",
    "\n",
    "# Get the header and add SAME_STATE\n",
    "header = next(csv.reader([rddPatents.first()]))\n",
    "header = [h.replace('\"', '') for h in header]   # remove quotes\n",
    "header.append(\"SAME_STATE\")\n",
    "\n",
    "# Organize the data into a table\n",
    "def show_table(header, rows, n=10, max_col_width=10):\n",
    "    # replace \"\" with \"null\" like Spark\n",
    "    def norm(x):\n",
    "        x = \"\" if x is None else str(x)\n",
    "        x = \"null\" if x.strip() == \"\" else x\n",
    "        return x\n",
    "\n",
    "    rows = rows[:n]\n",
    "    rows = [[norm(x) for x in r] for r in rows]\n",
    "\n",
    "    # compute widths\n",
    "    widths = []\n",
    "    for j, h in enumerate(header):\n",
    "        col_vals = [h] + [r[j] if j < len(r) else \"\" for r in rows]\n",
    "        w = min(max(len(v) for v in col_vals), max_col_width)\n",
    "        widths.append(w)\n",
    "\n",
    "    def fmt_row(r):\n",
    "        cells = []\n",
    "        for j, w in enumerate(widths):\n",
    "            v = r[j] if j < len(r) else \"\"\n",
    "            v = v if len(v) <= w else v[:w-3] + \"...\"\n",
    "            cells.append(v.ljust(w))\n",
    "        return \"| \" + \" | \".join(cells) + \" |\"\n",
    "\n",
    "    border = \"+-\" + \"-+-\".join(\"-\"*w for w in widths) + \"-+\"\n",
    "\n",
    "    print(border)\n",
    "    print(fmt_row(header))\n",
    "    print(border)\n",
    "    for r in rows:\n",
    "        print(fmt_row(r))\n",
    "    print(border)\n",
    "    print(f\"only showing top {len(rows)} rows\")\n",
    "\n",
    "# Show the table\n",
    "show_table(header, result_rdd_final, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
